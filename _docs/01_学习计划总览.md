# Open-AutoGLM 学习计划总览

## 📋 项目简介

**Open-AutoGLM** 是一个基于多模态大语言模型的 Android 手机智能助理框架，它通过视觉语言模型理解屏幕内容，结合 ADB 控制设备，实现自然语言驱动的手机自动化操作。

### 核心技术栈
- **AI 模型**: AutoGLM-Phone-9B (视觉语言模型)
- **设备控制**: Android Debug Bridge (ADB)
- **API 标准**: OpenAI 兼容接口
- **图像处理**: PIL/Pillow
- **编程语言**: Python 3.10+

---

## 🎯 学习目标

通过本学习计划，你将掌握：

1. **原理层面**: 多模态 Agent 系统的工作机制
2. **架构层面**: 模型-决策-执行的完整链路
3. **实现层面**: 屏幕理解、动作规划、ADB 控制
4. **设计层面**: 提示词工程、错误处理、状态管理
5. **工程层面**: 远程调试、多设备管理、系统集成

---

## 📚 学习路径（建议 5-7 天）

### 第一阶段：快速上手（1天）

**目标**: 运行起来，建立感性认识

1. **环境搭建** (2小时)
   - 安装 ADB 工具
   - 配置 Android 设备 (开发者模式 + USB 调试)
   - 安装 ADB Keyboard
   - 安装 Python 依赖

2. **模型部署** (2小时)
   - 下载 AutoGLM-Phone-9B 模型
   - 使用 vLLM 或 SGlang 启动服务
   - 测试模型 API 连通性

3. **运行示例** (2小时)
   - 运行 `main.py` 交互模式
   - 尝试简单任务: "打开微信"
   - 观察日志输出 (思考过程 + 执行动作)
   - 运行 `examples/basic_usage.py`

**输出**: 成功运行一个端到端的任务，看到 AI 如何思考和操作

---

### 第二阶段：核心原理（1-2天）

**目标**: 深入理解系统工作原理

#### Day 2: 架构与流程

1. **整体架构理解** (3小时)
   ```
   用户指令 → Agent 主循环 → 视觉理解 → 动作规划 → ADB 执行 → 状态反馈
   ```
   - 阅读: `_docs/02_核心原理解析.md`
   - 核心文件: `phone_agent/agent.py`
   - 绘制系统架构图

2. **多模态理解机制** (2小时)
   - 屏幕截图如何被编码
   - 图像与文本的融合方式
   - 历史对话的上下文管理
   - 文件: `phone_agent/model/client.py`

3. **Prompt Engineering** (2小时)
   - 阅读系统提示词: `phone_agent/config/prompts.py`
   - 分析提示词结构:
     - Think-Answer 格式
     - 动作指令定义
     - 规则约束设计
   - 尝试修改提示词观察效果

#### Day 3: 动作执行系统

1. **ADB 控制原理** (3小时)
   - ADB 命令系统
   - 坐标转换 (相对 → 绝对)
   - 文件清单:
     - `phone_agent/adb/device.py` - 设备控制
     - `phone_agent/adb/input.py` - 文本输入
     - `phone_agent/adb/screenshot.py` - 屏幕截图

2. **动作处理器** (2小时)
   - 文件: `phone_agent/actions/handler.py`
   - 支持的 12 种动作类型
   - 敏感操作确认机制
   - 人工接管流程

**输出**: 绘制完整的数据流图，理解每个组件的职责

---

### 第三阶段：代码精读（1-2天）

**目标**: 掌握实现细节和最佳实践

#### Day 4: 核心代码

1. **Agent 主循环** (3小时)
   - `PhoneAgent.run()` 方法解析
   - `_execute_step()` 单步执行逻辑
   - 上下文管理与内存优化
   - 错误处理与重试机制

2. **模型交互** (2小时)
   - `ModelClient.request()` 实现
   - OpenAI API 参数配置
   - Think-Answer 解析
   - 响应容错处理

3. **状态管理** (1小时)
   - 对话历史维护
   - 图像内容清理 (节省上下文)
   - 步数限制与循环检测

#### Day 5: 工程实践

1. **远程调试系统** (2小时)
   - 文件: `phone_agent/adb/connection.py`
   - WiFi ADB 连接原理
   - 多设备管理
   - 网络调试最佳实践

2. **应用管理** (1小时)
   - 文件: `phone_agent/config/apps.py`
   - 应用包名映射
   - 启动器实现
   - 如何添加新应用

3. **命令行工具** (1小时)
   - 文件: `main.py`
   - 参数解析与配置
   - 系统检查流程
   - 交互式模式实现

**输出**: 能够独立修改和扩展代码

---

### 第四阶段：精彩设计（1天）

**目标**: 学习值得借鉴的设计思想

#### Day 6: 设计亮点

阅读 `_docs/04_精彩设计解析.md`，重点学习:

1. **坐标归一化设计** (1小时)
   - 为什么使用 0-1000 坐标系
   - 跨分辨率适配的巧妙方案

2. **渐进式推理** (1小时)
   - Think-Answer 模式的优势
   - 可解释性与调试性

3. **错误恢复机制** (1小时)
   - 黑屏检测 → 敏感页面识别
   - 无效操作 → 自动重试
   - 卡住检测 → 规则引导

4. **提示词工程** (2小时)
   - 18 条行为规则
   - 任务导向的约束设计
   - 如何避免死循环

5. **上下文优化** (1小时)
   - 图像内容清理策略
   - 历史压缩技术

**输出**: 总结 3-5 个最值得学习的设计模式

---

### 第五阶段：实战改进（1-2天）

**目标**: 动手改进项目，加深理解

#### Day 7: 实战练习

参考 `_docs/05_改进方向与实践.md`，选择 1-2 个方向实践:

**初级实践**:
1. 添加一个新的动作类型 (如 `Pinch` 捏合)
2. 增加 10 个新应用的支持
3. 优化提示词，减少某个常见错误
4. 实现任务执行日志记录

**中级实践**:
1. 实现智能重试机制 (动作失败自动重试)
2. 添加屏幕元素检测 (OCR 或 UI 层级解析)
3. 实现任务中断与恢复
4. 优化内存使用 (更激进的历史清理)

**高级实践**:
1. 实现多任务并发执行
2. 添加学习能力 (从历史成功案例学习)
3. 实现自动化测试框架
4. 集成其他多模态模型 (GPT-4V, Gemini)

**输出**: 一个可用的改进版本 + 技术报告

---

## 🔍 学习方法建议

### 1. 代码阅读顺序

```
main.py (入口)
    ↓
phone_agent/agent.py (核心 Agent)
    ↓
phone_agent/model/client.py (模型交互)
    ↓
phone_agent/actions/handler.py (动作执行)
    ↓
phone_agent/adb/*.py (底层控制)
    ↓
phone_agent/config/prompts.py (提示词)
```

### 2. 调试技巧

1. **启用详细日志**: 确保 `verbose=True`
2. **单步执行**: 使用 `agent.step()` 逐步调试
3. **修改提示词**: 在 `prompts.py` 中添加调试信息
4. **截图检查**: 保存每步的屏幕截图
5. **模型输出**: 打印完整的 Think-Answer 内容

### 3. 实验建议

- **对比实验**: 修改一个参数，观察效果变化
- **极限测试**: 尝试复杂任务，找到系统边界
- **失败分析**: 收集失败案例，分析原因
- **性能测试**: 记录响应时间、成功率

### 4. 学习资源

**论文阅读**:
- AutoGLM 原论文: `arXiv:2411.00820`
- Vision-Language Models 综述

**相关项目**:
- AppAgent (类似的手机 Agent 项目)
- Android UI Automator (原生自动化)
- Selenium (Web 自动化，思想类似)

**技术文档**:
- ADB 官方文档
- OpenAI API 规范
- GLM-4V 模型文档

---

## 📊 学习检查清单

完成每个阶段后，确认你能回答这些问题:

### ☑️ 第一阶段
- [ ] ADB 的作用是什么？
- [ ] 为什么需要 ADB Keyboard？
- [ ] 模型的输入和输出是什么格式？

### ☑️ 第二阶段
- [ ] Agent 如何知道当前屏幕状态？
- [ ] Think-Answer 模式的优势是什么？
- [ ] 相对坐标如何转换为绝对坐标？
- [ ] 如何处理敏感页面（如支付）？

### ☑️ 第三阶段
- [ ] 上下文历史如何管理？
- [ ] 为什么要删除历史图像？
- [ ] 动作执行失败如何处理？
- [ ] 如何支持多设备？

### ☑️ 第四阶段
- [ ] 归一化坐标的设计理由？
- [ ] 提示词的 18 条规则有何作用？
- [ ] 如何避免 Agent 陷入死循环？
- [ ] 错误恢复的核心策略是什么？

### ☑️ 第五阶段
- [ ] 如何添加新的动作类型？
- [ ] 如何优化模型推理速度？
- [ ] 项目的主要局限在哪里？
- [ ] 你会如何改进这个系统？

---

## 🎓 进阶方向

学完本项目后，可以探索:

1. **多模态 AI**
   - 更强大的 VLM 模型
   - 视频理解 Agent
   - 跨模态检索

2. **Agent 系统**
   - 多 Agent 协作
   - 工具学习 (Tool Learning)
   - 规划与推理

3. **自动化测试**
   - UI 自动化测试
   - 智能测试用例生成
   - Bug 自动复现

4. **具身智能**
   - 机器人控制
   - 虚拟环境交互
   - 现实世界任务执行

---

## 📞 遇到问题？

1. **查看文档**: 项目 README 和本学习计划
2. **阅读代码注释**: 代码中有详细的文档字符串
3. **运行示例**: `examples/` 目录中的代码
4. **社区支持**: 项目微信社区

---

## 🎉 预期收获

完成本学习计划后，你将:

✅ 深入理解多模态 Agent 的工作原理  
✅ 掌握 ADB 自动化控制技术  
✅ 学会提示词工程的实践技巧  
✅ 具备开发类似系统的能力  
✅ 理解 AI 系统的工程化方法  

---

**祝学习愉快！🚀**

*最后更新: 2025-12-09*



# Open-AutoGLM 核心原理解析

## 🎯 系统架构概览

Open-AutoGLM 是一个**视觉语言模型驱动的手机自动化 Agent 系统**，它的核心是将多模态理解与设备控制相结合。

### 架构图

```
┌─────────────────────────────────────────────────────────────┐
│                      用户自然语言指令                          │
│              "打开小红书搜索美食攻略"                          │
└────────────────────────┬────────────────────────────────────┘
                         │
                         ▼
┌─────────────────────────────────────────────────────────────┐
│                     PhoneAgent 主循环                         │
│  ┌───────────────────────────────────────────────────────┐  │
│  │  Step 1: 截取当前屏幕 (Screenshot)                     │  │
│  │  Step 2: 获取当前应用 (Current App)                    │  │
│  │  Step 3: 构建多模态消息 (Text + Image)                 │  │
│  │  Step 4: 调用 VLM 模型推理                             │  │
│  │  Step 5: 解析 Think-Answer                            │  │
│  │  Step 6: 执行动作 (ADB Command)                       │  │
│  │  Step 7: 检查是否完成，否则返回 Step 1                  │  │
│  └───────────────────────────────────────────────────────┘  │
└────────────────────────┬────────────────────────────────────┘
                         │
          ┌──────────────┼──────────────┐
          │              │              │
          ▼              ▼              ▼
    ┌─────────┐   ┌──────────┐   ┌──────────┐
    │ 模型层   │   │ 动作层    │   │ 设备层    │
    │(VLM)    │   │(Handler) │   │(ADB)     │
    └─────────┘   └──────────┘   └──────────┘
```

---

## 🧠 核心原理 1: 视觉语言理解

### 1.1 屏幕感知机制

**问题**: Agent 如何"看到"手机屏幕？

**解决方案**: 
1. **屏幕截图**: 通过 ADB 命令 `screencap` 获取实时截图
2. **Base64 编码**: 将图像转换为 Base64 字符串
3. **多模态输入**: 图像 + 文本一起发送给 VLM

```python
# phone_agent/adb/screenshot.py
def get_screenshot(device_id):
    # 1. ADB 截图命令
    subprocess.run(["adb", "shell", "screencap", "-p", "/sdcard/tmp.png"])
    
    # 2. 拉取到本地
    subprocess.run(["adb", "pull", "/sdcard/tmp.png", temp_path])
    
    # 3. 编码为 Base64
    img = Image.open(temp_path)
    base64_data = base64.b64encode(img_bytes).decode("utf-8")
    
    return Screenshot(base64_data, width, height)
```

**关键设计**:
- **黑屏检测**: 如果截图失败（敏感页面），返回黑色图像 + `is_sensitive=True` 标记
- **尺寸记录**: 保存屏幕宽高，用于后续坐标转换

---

### 1.2 多模态消息构建

**问题**: 如何将屏幕信息传递给 AI？

**解决方案**: OpenAI 多模态消息格式

```python
# 第一步：系统提示词
{
    "role": "system",
    "content": "你是一个智能体分析专家..."  # 包含所有规则和动作定义
}

# 第二步：用户任务 + 屏幕
{
    "role": "user",
    "content": [
        {"type": "image_url", "image_url": {"url": "data:image/png;base64,iVBORw0KG..."}},
        {"type": "text", "text": "打开小红书搜索美食攻略\n\n** Screen Info **\n{\"current_app\":\"System Home\"}"}
    ]
}

# 第三步：AI 响应
{
    "role": "assistant",
    "content": "<think>当前在系统桌面...</think><answer>do(action=\"Launch\", app=\"小红书\")</answer>"
}

# 第四步：下一轮屏幕
{
    "role": "user",
    "content": [
        {"type": "image_url", "image_url": {"url": "data:image/png;base64,iVBORw0KG..."}},
        {"type": "text", "text": "** Screen Info **\n{\"current_app\":\"小红书\"}"}
    ]
}
```

**关键点**:
- **对话历史**: 保持完整的对话链，让 AI 知道之前做了什么
- **上下文管理**: 删除历史消息中的图像，只保留文本（节省 token）
- **屏幕信息**: 每轮携带当前应用名称，帮助 AI 定位

---

### 1.3 Think-Answer 模式

**问题**: 如何让 AI 的决策过程可解释？

**解决方案**: 强制 AI 先思考，再输出动作

```xml
<think>
当前在系统桌面，需要先启动小红书应用。
根据任务要求，先执行 Launch 动作。
</think>

<answer>
do(action="Launch", app="小红书")
</answer>
```

**优势**:
1. **可解释性**: 用户可以看到 AI 的推理过程
2. **调试友好**: 出错时可以分析思考是否正确
3. **思维链 (CoT)**: 提高复杂任务的规划能力

**实现**:
```python
# phone_agent/model/client.py
def _parse_response(self, content: str):
    thinking = content.split("<answer>")[0].replace("<think>", "").replace("</think>", "").strip()
    action = content.split("<answer>")[1].replace("</answer>", "").strip()
    return thinking, action
```

---

## ⚙️ 核心原理 2: 动作执行系统

### 2.1 动作类型设计

**12 种动作类型**:

| 动作 | 描述 | ADB 实现 |
|------|------|----------|
| `Launch` | 启动应用 | `adb shell monkey -p <package> 1` |
| `Tap` | 点击 | `adb shell input tap <x> <y>` |
| `Type` | 输入文本 | ADB Keyboard |
| `Swipe` | 滑动 | `adb shell input swipe <x1> <y1> <x2> <y2> <duration>` |
| `Back` | 返回 | `adb shell input keyevent 4` |
| `Home` | 主屏幕 | `adb shell input keyevent KEYCODE_HOME` |
| `Long Press` | 长按 | `adb shell input swipe <x> <y> <x> <y> 3000` |
| `Double Tap` | 双击 | 两次 `tap` 命令，间隔 0.1 秒 |
| `Wait` | 等待 | `time.sleep(duration)` |
| `Take_over` | 人工接管 | 回调函数 |
| `Note` | 记录内容 | (占位符) |
| `Call_API` | 调用 API | (占位符) |

---

### 2.2 坐标归一化 (关键设计)

**问题**: 不同手机屏幕分辨率不同，如何统一？

**解决方案**: **相对坐标系统 (0-1000)**

```
┌────────────────────────────┐
│ (0,0)            (1000,0)  │  手机屏幕
│                            │
│         (500,500)          │  中心点
│                            │
│ (0,1000)        (1000,1000)│
└────────────────────────────┘
```

**转换公式**:
```python
def _convert_relative_to_absolute(element, screen_width, screen_height):
    x = int(element[0] / 1000 * screen_width)   # 相对 → 绝对
    y = int(element[1] / 1000 * screen_height)
    return x, y
```

**示例**:
- AI 输出: `[500, 200]` (屏幕水平中央，靠上)
- 1080x2400 屏幕: `(540, 480)` 像素
- 1440x3200 屏幕: `(720, 640)` 像素

**优势**:
- 模型无需关心具体分辨率
- 同一个指令可以在不同设备上工作
- 提高模型泛化能力

---

### 2.3 文本输入方案

**问题**: ADB 原生输入不支持中文，且有转义问题

**解决方案**: **ADB Keyboard**

```python
# phone_agent/adb/input.py
def type_text(text, device_id):
    # 1. 切换到 ADB Keyboard
    original_ime = detect_and_set_adb_keyboard(device_id)
    
    # 2. 清空输入框
    clear_text(device_id)
    
    # 3. 通过广播发送文本
    subprocess.run([
        "adb", "shell",
        "am", "broadcast",
        "-a", "ADB_INPUT_TEXT",
        "--es", "msg", text
    ])
    
    # 4. 恢复原输入法
    restore_keyboard(original_ime, device_id)
```

**原理**:
- ADB Keyboard 是一个特殊的 Android 输入法
- 它监听 ADB 广播，直接将文本插入到输入框
- 支持任意 Unicode 字符（中文、emoji 等）

---

## 🔄 核心原理 3: Agent 主循环

### 3.1 执行流程

```python
# phone_agent/agent.py
def run(self, task: str) -> str:
    self._context = []  # 清空对话历史
    self._step_count = 0
    
    # 第一步：用户任务
    result = self._execute_step(task, is_first=True)
    if result.finished:
        return result.message
    
    # 循环执行直到完成或达到最大步数
    while self._step_count < self.max_steps:
        result = self._execute_step(is_first=False)
        if result.finished:
            return result.message
    
    return "Max steps reached"
```

### 3.2 单步执行详解

```python
def _execute_step(self, user_prompt=None, is_first=False):
    # 1. 获取当前状态
    screenshot = get_screenshot(device_id)         # 截图
    current_app = get_current_app(device_id)       # 当前应用
    
    # 2. 构建消息
    if is_first:
        messages.append(system_message)            # 系统提示词
        messages.append(user_message_with_image)   # 任务 + 屏幕
    else:
        messages.append(screen_message_with_image) # 仅屏幕状态
    
    # 3. 模型推理
    response = self.model_client.request(messages)
    thinking, action = parse_response(response)
    
    # 4. 执行动作
    result = self.action_handler.execute(action, screen_width, screen_height)
    
    # 5. 更新上下文
    messages.append(assistant_message)              # AI 的响应
    messages[-2] = remove_images(messages[-2])      # 删除上一轮的图像
    
    # 6. 检查是否完成
    finished = (action["_metadata"] == "finish") or result.should_finish
    
    return StepResult(...)
```

**关键点**:
1. **状态捕获**: 每步都重新截图，获取最新状态
2. **历史管理**: 保留完整对话，但删除历史图像
3. **错误处理**: 模型异常、动作失败都有容错
4. **终止条件**: `finish` 动作、最大步数、严重错误

---

### 3.3 上下文优化

**问题**: 对话历史会越来越长，token 消耗巨大

**解决方案**: **图像内容清理**

```python
# 只保留最新的一张图像
self._context[-1] = MessageBuilder.remove_images_from_message(self._context[-1])
```

**效果**:
- 保留: 所有文本内容（任务、思考、动作）
- 删除: 历史屏幕图像
- 结果: 上下文长度稳定增长，不会爆炸

**示例上下文**:
```
[System] 系统提示词
[User] 任务 + (已删除图像)
[Assistant] <think>...</think><answer>do(...)</answer>
[User] 屏幕信息 + (已删除图像)
[Assistant] <think>...</think><answer>do(...)</answer>
[User] 屏幕信息 + 🖼️ <当前图像>  ← 只有这个有图像
```

---

## 🛡️ 核心原理 4: 错误处理与容错

### 4.1 敏感页面检测

**场景**: 支付页面、密码输入时，Android 会阻止截图

**处理**:
```python
# phone_agent/adb/screenshot.py
result = subprocess.run(["adb", "shell", "screencap", "-p", "/sdcard/tmp.png"])

if "Status: -1" in result.stderr or "Failed" in result.stderr:
    # 截图失败 → 返回黑屏 + 标记
    return Screenshot(black_image, is_sensitive=True)
```

**Agent 响应**:
- 检测到 `is_sensitive=True`
- 自动执行 `Take_over` 动作
- 请求人工介入

---

### 4.2 动作失败重试

**场景**: 点击未生效、滑动不响应

**Prompt 规则**:
```
14. 在执行下一步操作前请一定要检查上一步的操作是否生效，
    如果点击没生效，可能因为 app 反应较慢，请先稍微等待一下，
    如果还是不生效请调整一下点击位置重试。
```

**实现**: 通过提示词引导 AI 自我纠错

---

### 4.3 死循环检测

**场景**: AI 可能陷入重复动作

**Prompt 规则**:
```
13. 执行任务过程中如果有多个可选择的项目栏，请逐个查找每个项目栏，
    直到完成任务，一定不要在同一项目栏多次查找，从而陷入死循环。
```

**未来改进**: 可以添加状态哈希检测，发现重复状态时强制切换策略

---

## 🎯 核心原理 5: Prompt Engineering

### 5.1 提示词结构

```
1. 角色定义: "你是一个智能体分析专家"
2. 输出格式: "<think>...</think><answer>...</answer>"
3. 动作定义: 12 种动作的详细说明
4. 行为规则: 18 条约束规则
```

### 5.2 关键规则解析

| 规则 | 目的 | 示例 |
|------|------|------|
| 规则 1 | 确保在正确的应用中执行任务 | 搜索前先检查是否在目标 App |
| 规则 2 | 避免进入错误页面 | 误触广告后立即返回 |
| 规则 3 | 防止无限等待 | 最多等 3 次，否则返回重试 |
| 规则 11 | 提高任务完成率 | 允许模糊匹配（如搜索"宠物友好餐厅"） |
| 规则 17 | 防止错误累积 | 尝试 3 次后放弃，而非无限循环 |

### 5.3 Few-Shot 思想

虽然没有显式的示例，但规则中包含了隐式的 few-shot:

```
8. 购物车全选后再点击全选可以把状态设为全不选，
   在做购物车任务时，如果购物车里已经有商品被选中时，
   你需要点击全选后再点击取消全选，再去找需要购买或者删除的商品。
```

这实际上是一个"如何清空购物车选中状态"的示例。

---

## 📊 系统运行示例

### 完整执行流程

**任务**: "打开小红书搜索美食攻略"

```
Step 1:
  📸 Screen: 系统桌面
  💭 Think: 当前在系统桌面，需要先启动小红书应用
  🎯 Action: do(action="Launch", app="小红书")
  ✅ Result: 启动小红书

Step 2:
  📸 Screen: 小红书首页
  💭 Think: 小红书已打开，需要点击顶部搜索框
  🎯 Action: do(action="Tap", element=[500, 80])
  ✅ Result: 点击搜索框

Step 3:
  📸 Screen: 搜索页面（输入框已激活）
  💭 Think: 搜索框已激活，现在输入"美食攻略"
  🎯 Action: do(action="Type", text="美食攻略")
  ✅ Result: 输入文本

Step 4:
  📸 Screen: 搜索页面（已有输入内容）
  💭 Think: 已输入文本，需要点击搜索按钮
  🎯 Action: do(action="Tap", element=[950, 80])
  ✅ Result: 点击搜索

Step 5:
  📸 Screen: 搜索结果页
  💭 Think: 搜索结果已展示，任务完成
  🎯 Action: finish(message="已成功搜索美食攻略")
  ✅ Result: 任务完成
```

---

## 🔑 核心技术要点总结

### 1. 多模态理解
- ✅ 图像 + 文本融合
- ✅ 对话历史管理
- ✅ 上下文优化

### 2. 动作执行
- ✅ 归一化坐标系统
- ✅ ADB 命令封装
- ✅ 中文输入支持

### 3. 智能规划
- ✅ Think-Answer 模式
- ✅ 规则引导推理
- ✅ 自我纠错能力

### 4. 工程实践
- ✅ 错误容错
- ✅ 远程调试
- ✅ 多设备支持

---

## 🚀 原理的延伸思考

### 为什么这个系统能工作？

1. **视觉语言模型的强大能力**: AutoGLM 能够理解复杂的 UI 布局
2. **坐标归一化的泛化性**: 无需为每个设备训练
3. **提示词的引导作用**: 18 条规则有效约束了 AI 行为
4. **闭环反馈**: 每步都能看到执行结果，及时调整

### 系统的局限性

1. **依赖视觉理解**: 无法处理纯文本的元素（可通过 UI 层级解析改进）
2. **响应延迟**: 每步需要模型推理（1-3 秒）
3. **token 消耗**: 图像编码占用大量 token
4. **错误累积**: 一步错误可能导致后续全错

### 改进空间

1. **结构化 UI**: 结合 UI 层级树 (UIAutomator)
2. **记忆机制**: 学习成功的操作序列
3. **并行执行**: 分解任务，多线程执行
4. **主动学习**: 从用户反馈中改进

---

**深入理解这些原理，你将能够构建自己的 Agent 系统！🎓**


